## java

### ==与equest的区别

```
== 操作符专门用来比较两个变量的值是否相等，也就是用于比较变量所对应的内存中所存储的数值是否相同，要比较两个基本类型的数据或两个引用变量是否相等，只能用==操作符。
equest方法是用于比较两个独立对象的内容是否相同。
```

### eqauls方法和hashCode方法的区别

```
1、相等（相同）的对象必须具有相等的哈希码（或者散列码）。
2、如果两个对象的hashCode相同，它们并不一定相同。
```

### 集合体系

``` 
                                        Collection

							List
ArrayList                    LinkedList						  Vector
底层的数据结构采用数组          排列有序,可重复				    排列有序,可重复
查询很快,但是增加删除很慢		底层使用双向循环链表数据结构		  底层使用数组
线程不同步					查询慢,增删快					   查询快,增删慢
排列有序,可重复			   线程不安全					   线程安全,效率低

						    Set
HashSet					   LinkedHashSet						TreeSet
排列无序,不可重复    采用Hash表存储,并用双向链表记录插入顺序	排列无序,不可重复
底层使用Hash表实现	    内部使用LinkedHashMap	       底层使用二叉树实现
存取速度快													   排列存储
允许插入Null值												   内部使用TreeMap的SortedSet
线程不安全                                                           允许插入Null值
内部是HashMap	
						    Queue
在两端出入的List,所以也可以使用数组和链表实现

										Map
	
HashMap						    Hashtable                    TreeMap				
键不可重复,值可重复			      键不可重复,值可重复		    键不可重复,值可重复	
底层使用哈希表					   底层使用哈希表				  底层为二叉树
线程不安全					     线程安全
允许key为null，value也可以为null	key,value都不允许为null
				
```

### Vector和ArrayList的区别和联系

```
Vector和ArrayList的区别和联系
实现原理相同，功能相同，都是长度可变的数组结构，很多情况下可以互用
两者的主要区别如下
1)Vector是早期JDK接口，ArrayList是替代Vector的新接口
2)Vector线程安全，ArrayList线程非安全
3)长度需增长时，Vector默认增长一倍，ArrayList增长50%
```

### ArrayList和LinkedList的区别和联系

```
两者都实现了List接口，都具有List中元素有序、不唯一的特点。
ArrayList实现了长度可变的数组，在内存中分配连续空间。遍历元素和随机访问元素的效率比较高；
LinkedList采用链表存储方式。插入、删除元素时效率比较高,效率高体现在当他对元素删除的时候只需要将上一个元素的指针指向被删除元素的后一个元素即可,查询效率不高的原因体现在他需要根据第一个元素一个一个向下查找
```

### HashMap和Hashtable的区别和底层实现

```
实现原理相同，功能相同，底层都是哈希表结构，查询速度快，在很多情况下可以互用
两者的主要区别如下
1)Hashtable是早期JDK提供的接口，HashMap是新版JDK提供的接口
2)Hashtable继承Dictionary类，HashMap实现Map接口
3)Hashtable线程安全，HashMap线程非安全
4)Hashtable不允许null值，HashMap允许null值

底层实现:数组+链表实现
jdk8开始链表高度到8的,数据长度超过64,链表转变为红黑树,元素以内部类Node节点存在
*计算key的hash值,二次hash然后对数组长度进行取模,对应到数组小标
*如果没有产生hash冲突(下标位置没有元素),则直接创建Node存入数组
*如果产生hash冲突,先进行equal比较,相同则取代该元素,不同,则判断链表高度插入链表,链表高度达到8
,并且数组长度到64则转变为红黑树,长度低于6则将红黑树转回链表
*key为null，存在下标0的位置
```

### ConcurrentHashMap原理,jdk7和jdk8版本的区别

```
jdk7:
数据结构:ReentrantLock+Segment+HashEntry,底层一个Segments数组，存储一个Segments对象，一个Segments中储存一个Entry数组，存储的每个Entry对象又是一个链表头结点。初始化有三个参数：initialCapacity：初始容量大小 ，默认16。loadFactor, 扩容因子，默认0.75，当一个Segment存储的元素数量大于initialCapacity* loadFactor时，该Segment会进行一次扩容。concurrencyLevel 并发度，默认16。并发度可以理解为程序运行时能够同时更新ConccurentHashMap且不产生锁竞争的最大线程数，即ConcurrentHashMap中的分段锁个数，即Segment[]的数组长度。

元素查询:1、第一次哈希 找到 对应的Segment段，调用Segment中的get方法
		2、再次哈希找到对应的链表，
		3、最后在链表中查找。
元素插入:
		当执行put方法插入数据的时候，先通过hash值在segment中找到对应的位置，然后如果相应位置的segment还未初始化，则通过CAS进行赋值，接着执行segment对象的put方法通过加锁机制插入数据。

锁:segment分段锁 Segment继承了ReentranLock,锁定操作的Segment,其他的Segment不受影响,并发度为segment个数,可以通过构造函数指定,数组扩容不会影响其他的segment
get方法无需加锁,volatile保证,保证可见性


JDK1.8：已摒弃Segment的概念，直接用Node数组+链表+红黑树的数据结构来实现，并发控制使用Synchronized和CAS来操作，是优化过且线程安全的HashMap。在JDK1.8中还能看到Segment的数据结构，但是已经简化了属性，只是为了兼容旧版本。

JDK1.8取消了segment数组，直接用table保存数据，锁的粒度更小，减少并发冲突的概率；
JDK1.8存储数据时采用了链表+红黑树的形式,性能提升很大；
JDK1.8的实现降低锁的粒度，JDK1.7版本锁的粒度是基于Segment的，包含多个HashEntry，而JDK1.8锁的粒度就是HashEntry（首节点）；
JDK1.8版本的数据结构变得更加简单，使得操作也更加清晰流畅，已使用synchronized来进行同步，由于粒度的降低，实现的复杂度增加；
JDK1.8使用红黑树来优化链表，红黑树的遍历效率是很快；
JDK1.8使用内置锁synchronized来代替重入锁ReentrantLock。
```

### 接口的幂等性实现

```
1.唯一id,每次操作,都根据操作和内容生成唯一的id,在执行之前先判断是否存在,如果不存在则执行后续操作,并且保存到数据库或者redis等
2.服务端提供发送token的接口,业务调用接口前先获取token,然后调用业务请求时把token携带过去,服务器判断token是否存在redis中,存在表示第一次请求,可以继续执行业务,执行业务完成后,最后把redis中的token删除
3.版本控制 增加版本号,当版本号符合,才能更新数据
```

### 集合和数组的比较（为什么引入集合）

```
数组不是面向对象的，存在明显的缺陷，集合完全弥补了数组的一些缺点，比数组更灵活更实用，可大大提高软件的开发效率而且不同的集合框架类可适用于不同场合。具体如下：
1)数组的效率高于集合类.
2)数组能存放基本数据类型和对象，而集合类中只能放对象。
3)数组容量固定且无法动态改变，集合类容量动态改变。 
4)数组无法判断其中实际存有多少元素，length只告诉了array的容量。
5)集合有多种实现方式和不同的适用场合，而不像数组仅采用顺序表方式。
6)集合以类的形式存在，具有封装、继承、多态等类的特性，通过简单的方法和属性调用即可实现各种复杂操作，大大提高软件的开发效率。
```

### HashSet的使用和原理（hashCode()和equals()）

```
1)哈希表的查询速度特别快，时间复杂度为O（1）。
2)HashMap、Hashtable、HashSet这些集合采用的是哈希表结构，需要用到hashCode哈希码，hashCode是一个整数值。
3)系统类已经覆盖了hashCode方法 自定义类如果要放入hash类集合，必须重写hashcode。如果不重写，调用的是Object的hashcode，而Object的hashCode实际上是地址。
4)向哈希表中添加数据的原理：当向集合Set中增加对象时，首先集合计算要增加对象的hashCode码，根据该值来得到一个位置用来存放当前对象，如在该位置没有一个对象存在的话，那么集合Set认为该对象在集合中不存在，直接增加进去。如果在该位置有一个对象存在的话，接着将准备增加到集合中的对象与该位置上的对象进行equals方法比较，如果该equals方法返回false,那么集合认为集合中不存在该对象，在进行一次散列，将该对象放到散列后计算出的新地址里。如果equals方法返回true，那么集合认为集合中已经存在该对象了，不会再将该对象增加到集合中了。
5)在哈希表中判断两个元素是否重复要使用到hashCode()和equals()。hashCode决定数据在表中的存储位置，而equals判断是否存在相同数据。
6)	Y=K(X) ：K是函数，X是哈希码，Y是地址
```

### Collection和Collections的区别

```
Collection是Java提供的集合接口，存储一组不唯一，无序的对象。它有两个子接口List和Set。
Java中还有一个Collections类，专门用来操作集合类 ，它提供一系列静态方法实现对各种集合的搜索、排序、线程安全化等操作。
```

### IO

#### IO的几种分类

```
在java中，将不同的输入输出源通过流的形式进行相关操作(输入，输出)，流是一种抽象描述，在程序中表示数据的一种转移方式

Jdk提供了各种不同的流用于处理不同的输入输出源，根据流性质划分分为以下类型:

按流向分（站在程序角度考虑）

输入流(input)
输出流(output)

按类型分:

字节流(InputStream/OutputStream)
任何文件都可以通过字节流进行传输。

字符流(Reader/Writer)
非纯文本文件，不能用字符流，会导致文件格式破坏，不能正常执行。

节点流(低级流:直接跟输入输出源对接)
FileInputStream/FileOutputStream/FileReader/FileWriter/PrintStream/PrintWriter.

处理流(高级流:建立在低级流的基础上)
转换流：InputStreamReader/OutputStreamWriter，字节流转字符流/字符流转字节流

缓冲流：BufferedInputStream/BufferedOutputStream   BufferedReader/BufferedReader可对节点流经行包装，使读写更快
```



### 网络编程

#### tcp协议和udp协议的差别 

```

           TCP           UDP 
是否连接    面向连接     面向非连接 
传输可靠性    可靠        不可靠 
应用场合    少量数据    传输大量数据 
速度         慢         快

TCP保证数据正确性，UDP可能丢包，TCP保证数据顺序，UDP不保证。

TCP与UDP基本区别
  1.基于连接与无连接
  2.TCP要求系统资源较多，UDP较少； 
  3.UDP程序结构较简单 
  4.流模式（TCP）与数据报模式(UDP); 
  5.TCP保证数据正确性，UDP可能丢包 
  6.TCP保证数据顺序，UDP不保证 
```

### 多线程

#### 线程的创建方式
```

```
#### volatile

```
volatile是Java提供的一种轻量级的同步机制。Java 语言包含两种内在的同步机制：同步块（或方法）和 volatile 变量，相比于synchronized
（synchronized通常称为重量级锁），volatile更轻量级，因为它不会引起线程上下文的切换和调度。但是volatile 变量的同步性较差（有时它更简单并且开销更低），而且其使用也更容易出错。
```

#### synchronized和Lock的特点

```
synchronized和Lock既是同步锁又是可重入锁
1.首先synchronized是java内置关键字，在jvm层面，Lock是个java类；
2.synchronized无法判断是否获取锁的状态，Lock可以判断是否获取到锁；
3.synchronized会自动释放锁(a 线程执行完同步代码会释放锁 ；b 线程执行过程中发生异常会释放锁)，Lock需在finally中手工释放锁（unlock()方法释放锁），否则容易造成线程死锁；
4.用synchronized关键字的两个线程1和线程2，如果当前线程1获得锁，线程2线程等待。如果线程1阻塞，线程2则会一直等待下去，而Lock锁就不一定会等待下去，如果尝试获取不到锁，线程可以不用一直等待就结束了；
5.synchronized的锁可重入、不可中断、非公平，而Lock锁可重入、可判断、可公平（两者皆可）
6.Lock锁适合大量同步的代码的同步问题，synchronized锁适合代码少量的同步问题。
7.最重要的是Lock是一个接口，而synchronized是一个关键字，synchronized放弃锁只有两种情况：①线程执行完了同步代码块的内容②发生异常；而lock不同，它可以设定超时时间，也就是说他可以在获取锁时便设定超时时间，如果在你设定的时间内它还没有获取到锁，那么它会放弃获取锁然后响应放弃操作。
```

#### 虚假唤醒

```
当一个条件满足时，很多线程都被唤醒了，但是只有其中部分是有用的唤醒，其它的唤醒都是无用功
1.比如说买货，如果商品本来没有货物，突然进了一件商品，这是所有的线程都被唤醒了，但是只能一个人买，所以其他人都是假唤醒，获取不到对象的锁
	
虚假唤醒：意思就是,wait()方法需要在循环中使用，以避免实现虚假唤醒，如果使用在if中,那下一次唤醒之后直接就执行后面的代码了，应当使用再while循环中再一次判断
```



#### 多线程中锁的种类

```
------------------------------------------------------公平锁和非公平锁--------------------------------------------------------------------
公平锁：是指多个线程按照申请锁的顺序来获取锁
非公平锁：是指多个线程获取锁的顺序并不是按照申请锁的顺序，有可能后申请的线程比先申请的线程优先获锁。
非公平锁一上来就尝试占用锁，如果尝试占用失败，就采用公平锁的方式到末尾排队。
在高并发的情况下，有可能造成优先级反转或饥饿现象
非公平锁的优点在于吞吐量比公平锁大。
ReentrantLock：可以指定构造方法的boolean类型来指定是公平锁还是非公平锁，默认是非公平锁
synchronized：是一种非公平锁

-------------------------------------------------------------可重入锁--------------------------------------------------------------------
可重入锁：指的是同一线程外层方法获得锁之后，内层递归方法仍然能够获得该锁的代码
在同一个线程在外层方法获取锁的时候，在进入内层方法的时候会自动获取锁
也就是说，线程可以进入任何一个它自己已经拥有的锁所同步着的代码块
可重入锁的最大作用是避免死锁
ReentrantLock和synchronized就是一个典型的可重入锁

-------------------------------------------------------------自旋锁--------------------------------------------------------------------
是指尝试获取锁的线程不会立即被阻塞，而是采用循环的方式去尝试获取锁，这样的好处是减少线上下文切换的消耗，缺点是循环会消耗CPU
例如：CAS中就使用了自旋锁
自旋锁是使用CPU时间换取线程阻塞与调度的开销，但是很有可能这些CPU时间白白浪费了。   

-------------------------------------------------------------独占锁--------------------------------------------------------------------
是指该锁一次只能被一个线程所持有。ReentrantLock和synchronized都是独占锁

-------------------------------------------------------------共享锁--------------------------------------------------------------------
是指该锁可以被多个线程所持有ReentrantReadWriteLock：其读锁是共享锁，其写锁是独占锁
读锁的共享锁可保证并发读是非常高效的，读写、写读、写写的过程是互斥的

-------------------------------------------------------------死锁----------------------------------------------------------------------
死锁：当一个线程永远的持有一把锁，并且其他线程都尝试来获得这把锁时，就会发生死锁。
多个线程互相拥有锁，互不释放锁，造成线程死锁。可以通过cmd命令窗口中输入jconsole命令来检测线程情况，查看死锁。

------------------------------------------------------------读写锁----------------------------------------------------------------------
读写锁既是排它锁又是共享锁。读锁（readLock）是共享锁，写锁（writeLock）是排他锁。        
排它锁：在同一时刻，只允许一个线程进行访问。          
共享锁：在同一时刻，可以允许多个线程进行访问。共享锁与共享锁之间可以同时访问的，比如可以有多个读线程进行同时读。     
在只有读线程的条件下，多个线程可以同时进入这个方法。读线程没有线程安全问题，可以保证读的性能提高。

锁降级：是指写锁降级为读锁。   
实现：在写锁没有释放的时候，获取到读锁，再释放写锁。    
锁升级：是指把读锁升级为写锁。   
实现：在读锁没有释放的时候，获取到写锁，再释放读锁。     
ReentrantReadWriteLock不支持锁升级，只支持锁降级。    

------------------------------------------------------------悲观锁----------------------------------------------------------------------
总是假设最坏的情况，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会阻塞直到它拿到锁。这样可以保证每次都只有一个线程在访问这个数据；传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。再比如Java里面的同步原语synchronized关键字的实现也是悲观锁。

------------------------------------------------------------乐观锁----------------------------------------------------------------------
很乐观，每次去拿数据的时候都认为别人不会修改，所以不会上锁，那么就会有很多对象可以同时访问这个锁里面的数据，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号等机制。乐观锁适用于多读的应用类型，这样可以提高吞吐量。

------------------------------------------------------------可中断锁----------------------------------------------------------------------
如果某一线程A正在执行锁中的代码，另一线程B正在等待获取该锁，可能由于等待时间过长，线程B不想等待了，想先处理其他事情，我们可以让它中断自己或者在别的线程中中断它，这种就是可中断锁。在Java中，synchronized就不是可中断锁，而Lock是可中断锁。

------------------------------------------------------------活锁----------------------------------------------------------------------
是指线程1可以使用资源，但它很礼貌，让其他线程先使用资源，线程2也可以使用资源，但它很绅士，也让其他线程先使用资源。这样你让我，我让你，最后两个线程都无法使用资源。


```

## Mysql

### 存储引擎

```
innodb 支持事务,支持外键,是行级锁,索引和数据是存储在一起的 B+数,支持高并非(分库分表,读写分离),高可用(主备切换)
mylsam 不支持事务,不支持外键,是表级锁,索引和数据是分开的,查询性能比较好 B+树,一般用于报表系统,数据从hive导入mysql中,一次导入以后只做查询,一般用mylsam。现在的技术一般用es替代了mysql
```

### 索引的类型

```
mysql的索引分为单列索引(主键索引,唯一索引,普通索引)和组合索引
单列索引:一个索引只包含一个列,一个表可以有多个单列索引
组合索引:一个组合索引包含两个或两个以上的列
全文索引:
```

### 数据库和SQL优化
```
1）选择合适的数据库引擎

 MyIsam和Innodb的区别

2）合理的创建索引

 要在什么字段上创建索引：where后面的条件上，在order by的字段上，在需要关联查询的关联字段上创建索引；
 什么字段上一般不要创建索引：在值分布均匀的字段上不要创建索引（性别），不要在大文本字段上创建索引；
 慎用组合索引，建表后先建索引，不要等表里有数据后再添加索引。
 索引避免了全表扫描，提高了查询速度，但是索引并不是越多越好，因为索引的存储需要占用空间，对表的增删改操作也需要操作索引，浪费了资源和效率；

3）如何命中索引,避免放弃使用索引而进行全表扫描（SQL的优化）

 where条件尽量不要使用not, in, or, null，<,>;
 模糊查询时，匹配符要放在后面，例如like ‘chai%’
 where条件中不要有函数和表达式；
 如果是组合索引，要满足最左匹配原则；

4）sql的其它优化

 尽量使用varchar/nvarchar, 不要使用char/nchar;
 尽量不要用*，  select * from ……用具体的字段； select count(*) from …. … 用count(id)
 SQL尽量大写
 group by 之前进行条件过滤然后再分组；

5）执行过程分析SQL

 开发后打开慢日志；1S
 我们针对执行速度慢的SQL进行执行过程的分析；
 EXPLAIN sql语句

6) 提高高并发和高可用

 分库分表，读写分离提高高并发
 主备自动切换 实现高可用
```

### 数据库的传播特性
```
此特性是确保事务是否打开，业务逻辑是否使用同一事务的保证。 事务在传播过程中受到影响。 传播特性如下

1、Propagation.REQUIRED

调用方法时自动打开事务，如果在事务范围内使用，则使用相同的事务，否则打开新事务。

2、Propagation.REQUIRES_NEW

随时自己打开事务

3、Propagation.SUPPORTS

自己不打开事务。 在事务范围内使用相同的事务，否则不使用事务

4、Propagation.NOT_SUPPORTED

自己不打开事务，而是在事务范围内使用待定事务，并在执行完成后恢复事务

5、Propagation.MANDATORY

不自己打开事务，必须在事务环境中使用才能报告错误

6、Propagation.NEVER

自己不打开事务，而是在事务范围内使用抛出异常

7、Propagation.NESTED

如果存在活动事务，它将在嵌套事务中执行。 如果没有活动事务，则根据transaction definition.propagation _ required属性执行。 需要JDBC3.0或更高版本的支持。
```

### 数据库的隔离级别
```

```
## Spring

### Bean的初始化

```

---------------------------------Bean的定义--------------------------
（通过xml、注解、java配置）会被封装成BeanDefinition对象，该对象包括如下元数据：

1、类的全限定名

2、Bean行为配置元素，例如（scope、lifecycle callbacks等）

3、对于其他bean的依赖

4、其他配置信息，例如数据库连接池中连接的个数

---------------------------------Bean的属性--------------------------

scope：Bean的范围
constructor argument：构造函数参数
properties：所含有的属性
autowiring mode：自动装配的方式（根据类型、名字等进行自动装配）
lazy—initialization mode：延迟实例化，即在需要使用的时候实例化，而不是容器初始化时初始化
initialization method：初始化方法（不是构造函数）
destruction method：销毁方法



----------------------Bean的初始化有两种方式：---------------------------

1、通过构造函数：

spring可以管理任何形式的bean，不局限于javaBean(即提供一个默认构造函数，给每个属性一个get和set方法)

2、通过静态的工厂方法：
```

### Spring的一些常用注解

```java
使用注解前需要在xml中开启注解的支持

<context:annotation-config/>
    				  常用的spring注解
-----------------------------------------------------------------------------
@Required：应用在setter方法下，表示该属性不允许为空，如果为空，容器将会抛出异常
///////////////////////////////////////////////////////////////////////////
public class SimpleMovieLister {
    private MovieFinder movieFinder;
 
    @Required
    public void setMovieFinder(MovieFinder movieFinder) {
        this.movieFinder = movieFinder;
    }
}
///////////////////////////////////////////////////////////////////////////
@Autowired：注解按照bean的类型自动装配，不在赘述
@Primary：当有多个bean满足匹配结果时，具有@Primary注解的bean将被使用
@Qualifier：与@Primary不同，@Qualifier的使用更加灵活，通过指定bean的标识符，@Qualifier在有多种匹配结果的情况下，会通过bean的名字进行优先匹配,一般配合@Autowired注解一起使用
@Resource：与使用类型匹配的@Autowired不同，@Resource使用名字匹配，@Resource只能用于属性和setter方法上 
@PostConstruct：在bean初始化前先先调用该方法
@PreDestroy：在bean销毁对象前先调用该方法
@Component、@Service、@Controller、@Repository会被扫描注册为组件，DAO层使用注解@Repository，服务层使用@Service，控制层使用@Controller，这三个角色均可以使用@Component注解，但是不够细致化，@Service、@Controller、@Repository注解都是在@Component注解的基础上演变来的
@Scope、组件默认的生命周期是singleton，可以通过@Scope注解更改
@Lazy、懒加载（延迟加载）      单实例bean：默认在容器启动时创建对象   单实例只会创建一次    
                             懒加载：容器启动不创建对象，第一次获取Bean创建对象，并初始化 
    
@ComponentScan、用于扫描标包下面有注解的的类,@ComponentScan注解的scopedProxy属性指明代理的方式,该属性有三种值：no，interface（使用JDK代理），targetClass（使用CGLIB代理）
```



### 描述一下Spring AOP

```
指在程序运行期间动态的将某段代码切入到指定方法指定位置进行运行的编程方式，springaop通过代理模式实现方法的拦截
```

### SpringAop的代理方式

```
springaop没有默认的代理方式,他的代理方式主要看的是, 被代理的对象是类还是接口,如果是接口的话就默认使用的是java动态代理(JDK动态代理),也可以强制使用CGlib实现AOP的代理方式,如果是类的话那么就是CGlib代理,spring会在动态代理和CGlib之间转换,java动态代理比CGlib速度要快,但性能不如CGlib好,所以选择用哪种代理方式还是要看具体的情况,一般单例模式用CGlib比较好。
动态代理有两种
1.Java动态代理（JDK动态代理）（被代理的是接口）
2.CGlib代理（被代理的是类）
```

### 使用注解实现SpringAop的功能

```java
//表示这是被注入Spring容器中的
@Component
//表示这是个切面类
@Aspect
public class AnnotationHandler {
/*
* 在一个方法上面加上注解来定义切入点
* 这个切入点的名字就是这个方法的名字
* 这个方法本身不需要有什么作用
* 这个方法的意义就是:给这个 @Pointcut注解一个可以书写的地方
* 因为注解只能写在方法、属性、类的上面,并且方法名作为切入点的名字
* */

//简单来说就是将查到的方法用myPointCut()方法名代替，也就是抽取公共的切入点表达式,不抽取的话就要一个一个在增强方法上写切入点表达式,比如@Before("public * com.briup.aop.service..*.*(..))"),所代表的意思是(权限修饰符 方法的返回值 包名.类名(参数名))
@Pointcut("execution(public * com.briup.aop.service..*.*(..))")
public void myPointCut(){}

//注:这里面的所有方法的JoinPoint类型参数都可以去掉不写,如果确实用不上的话
@Before("myPointCut()")//在myPointCut()中查到的方法之前切入
public void beforeTest(JoinPoint p){
System.out.println(p.getSignature().getName()+" before...");
}

/*
* @After和@AfterReturning
* 
* @After标注的方法会在切入点上的方法结束后被调用(不管是不是正常的结束).
* @AfterReturning标注的方法只会在切入点上的方法正常结束后才被调用.
* */
@After("myPointCut()")//在myPointCut()中查到的方法之后切入
public void afterTest(JoinPoint p){
System.out.println(p.getSignature().getName()+" after...");
}
@AfterReturning("myPointCut()")
public void afterReturningTest(JoinPoint p){

System.out.println(p.getSignature().getName()+" afterReturning");

}

@Around("myPointCut()")//在myPointCut()中查到的方法环绕切入
public Object aroundTest(ProceedingJoinPoint pjp)throws Throwable{
System.out.println(pjp.getSignature().getName()+" is start..");
//调用连接点的方法去执行
Object obj = pjp.proceed();
System.out.println(pjp.getSignature().getName()+" is end..");
return obj;
}


//在切入点中的方法执行期间抛出异常的时候,会调用这个 @AfterThrowing注解所标注的方法
@AfterThrowing(value="myPointCut()",throwing="ex")
public void throwingTest(JoinPoint p,Exception ex){
System.out.println(p.getSignature().getName()+" is throwing..."+ex.getMessage());

}

}
```

```
spring的xml需要配置的东西

<!-- 让Spring扫描注解 -->
<context:component-scan base-package="com.briup.aop"></context:component-scan>
<!-- 识别AspectJ的注解 -->
<aop:aspectj-autoproxy/>
```

```
还需要在配置类上个标注一下@EnableAspectJAutoProxy，代表开启切面自动代理

@EnableAspectJAutoProxy 声明 proxyTargetClass 属性为 ture才是强制使用 cglib动态代理，proxyTargetClass属性默认时fals
```

### SpringAOP的基本术语

```
1、切面：拦截器类，其中会定义切点以及通知

2、通知：切面当中的方法，包括：

前置通知：在动态代理反射原先方法前调用的方法
后置通知：在动态代理反射完原有方法后调用的方法
返回通知：如果方法正常执行，在调用完后置通知后，就调用返回通知
异常通知：如果方法出现异常，在调用完后置通知后，就调用异常通知
环绕通知：可以决定是否调用目标方法，同时可以控制方法的返回对象（这个通知具有一些细节，接下来会进一步说明）
3、引入：往代理对象中添加新的方法，但是新的方法不会被拦截

4、切点：即定义需要拦截的方法的特征，可以通过正则表达式匹配，也可以通过类的全限定名

5、连接点：需要拦截的方法

6、织入：生成代理对象并将切面内容嵌入流程中，将切面内容嵌入到流程中是什么意思呢？例如现在定义了前置通知，那么代理对象在调用被代理对象的方法之前就会调用前置通知
```

```
@Aspect：表明这个类是一个切面

@Before：前置通知

@After：后置通知

@AfterReturning：正常通知

@AfterThrowing：异常通知

@Pointcut：切点

当有多个通知应用于同一个连接点时，通知的执行顺序是不确定的，可以在切面类中使用@Order(优先级)来指定执行顺序，此时spring会将多个通知组织成责任链的模式
```

## SpringMvc

### SpringMvc工作流程
```
  1.用户向服务器发送请求，请求被DispatcherServlet捕获；

  2.DispatcherServlet对请求URL进行解析，得到请求资源标识符（URI）。然后根据该URI，调用HandleMapping获得该Handler配置的所有相关的对象（包括Handler对象以及Handler对象对应的拦截器），最后以HandlerExecutionChain对象的形式返回；

  3.DispatcherServlet 根据获得的HandlerExecutionChain，通过存放在其中的handler选择一个合适的HandlerAdapter，如果有拦截器，则会织入拦截器的方法

  4.HandleAdapter提取Request中的模型数据，进行相应的类型转换，填充Handler入参，开始执行Handler（Controller)。 在填充Handler的入参过程中，根据你的配置，Spring将帮你做一些额外的工作：

      数据转换：对请求消息进行数据转换。如String转换成Integer、Double等，使用HttpMessageConveter（负责HTTP请求参数与java对象之间的相互转换），HttpMessageConveter只能进行文件类型以及String类型的简单转换，需要进一步转换才能成为POJO或者其他参数类型，为此，Spring提供了转换器和格式化器

      数据格式化：对请求消息进行数据格式化。 如将字符串转换成格式化数字或格式化日期等

      数据验证： 验证数据的有效性（长度、格式等），验证结果存储到BindingResult或Error中

  5.Handler执行完成后，向DispatcherServlet 返回一个ModelAndView对象；

  6.根据返回的ModelAndView，选择一个适合的ViewResolver（必须是已经注册到Spring容器中的ViewResolver)返回给DispatcherServlet ；

  7.ViewResolver 结合Model和View，来渲染视图

  8.将渲染结果返回给客户端。
  
```

## RabbitMQ
### 为什么需要消息队列
```
l.解耦（应用之间不再直接相互访问，而是直接与消息对列对接，用MQ消息队列，可以隔离系统上下游环境变化带来的不稳定因素，比如积分服务的系统需求无论如何变化，交易服务不用做任何改变，即使当积分服务出现故障，主交易流程也可以将积分服务降级，实习交易服务和积分服务的解耦，做到了系统的高可用)
2.异步（分解一个费时的操作，把它变成多个异步执行的步骤）
3.削峰（让处理程序相对均衡地处理数据，遇到秒杀等流量突增的场景，通过MQ还可以实现流量的“消峰填谷”的作用，可以根据下游的处理能力自动调节流量）
```
### 如何防止消息丢失
```
你要分析其中有几个考点，哪些环节可能会丢失消息？如何知道有消息丢失？如何确保消息不丢失？就好比“架构设计” “架构”体现了架构师的思考过程，而“设计”才是最后的解决方案，两者缺一不可。

《哪些环节可能会丢失消息》
【消息生产阶段，消息存储阶段，消息消费阶段】

<<<生产者没有成功把消息发送到MQ

丢失的原因：因为网络传输的不稳定性，当生产者在向MQ发送消息的过程中，MQ没有成功接收到消息，但是生产者却以为MQ成功接收到了消息，不会再次重复发送该消息，从而导致消息的丢失。

解决办法： 有两个解决办法：事务机制和confirm机制，最常用的是confirm机制。
 RabbitMQ可以开启 `confirm` 模式，在生产者那里设置开启 `confirm` 模式之后，生产者每次写的消息都会分配一个唯一的 id，如果消息成功写入 RabbitMQ 中，RabbitMQ 会给生产者回传一个 `ack` 消息，告诉你说这个消息 ok 了。如果 RabbitMQ 没能处理这个消息，会回调你的一个 `nack` 接口，告诉你这个消息接收失败，生产者可以发送。而且你可以结合这个机制自己在内存里维护每个消息 id 的状态，如果超过一定时间还没接收到这个消息的回调，那么可以重发。

RabbitMQ的事务机制是同步的，很耗型能，会降低RabbitMQ的吞吐量。confirm机制是异步的，生成者发送完一个消息之后，不需要等待RabbitMQ的回调，就可以发送下一个消息，当RabbitMQ成功接收到消息之后会自动异步的回调生产者的一个接口返回成功与否的消息。

<<<RabbitMQ接收到消息之后丢失了消息
丢失的原因：RabbitMQ接收到生产者发送过来的消息，是存在内存中的，如果没有被消费完，此时RabbitMQ宕机了，那么再次启动的时候，原来内存中的那些消息都丢失了。

解决办法：开启RabbitMQ的持久化。当生产者把消息成功写入RabbitMQ之后，RabbitMQ就把消息持久化到磁盘。结合上面的说到的confirm机制，只有当消息成功持久化磁盘之后，才会回调生产者的接口返回ack消息，否则都算失败，生产者会重新发送。存入磁盘的消息不会丢失，就算RabbitMQ挂掉了，重启之后，他会读取磁盘中的消息，不会导致消息的丢失。

持久化的配置：

-   第一点是创建 queue 的时候将其设置为持久化，这样就可以保证 RabbitMQ 持久化 queue 的元数据，但是它是不会持久化 queue 里的数据的。
-   第二个是发送消息的时候将消息的 `deliveryMode` 设置为 2，就是将消息设置为持久化的，此时 RabbitMQ 就会将消息持久化到磁盘上去。

注意：持久化要起作用必须同时设置这两个持久化才行，RabbitMQ 哪怕是挂了，再次重启，也会从磁盘上重启恢复 queue，恢复这个 queue 里的数据。

<<<消费者弄丢了消息
丢失的原因：如果RabbitMQ成功的把消息发送给了消费者，那么RabbitMQ的ack机制会自动的返回成功，表明发送消息成功，下次就不会发送这个消息。但如果就在此时，消费者还没处理完该消息，然后宕机了，那么这个消息就丢失了。

解决的办法：简单来说，就是必须关闭 RabbitMQ 的自动 `ack`，可以通过一个 api 来调用就行，然后每次在自己代码里确保处理完的时候，再在程序里 `ack` 一把。这样的话，如果你还没处理完，不就没有 `ack`了？那 RabbitMQ 就认为你还没处理完，这个时候 RabbitMQ 会把这个消费分配给别的 consumer 去处理，消息是不会丢的。
```
### 防止消息重复消费和幂等性
```
保证消息的唯一性，就算是多次传输，不要让消息的多次消费带来影响；保证消息等幂性；

《《《使用Redis解决重复消费和幂等性问题
在发送消息之前首先设置先在redis添加一条记录，记录当前消息的状态并设置一个UUID,在发送的消息的时候附带上这个UUID，在消费消息的时候先去redis中查询一下这个消息的状态，如果被消费过就不再执行业务逻辑，这样即使消息发送多次，也不会照成消息的重复消费，以及保证了幂等性。

幂等性还可以通过
1.  如果消息是做数据库的insert操作，给这个消息做一个唯一主键，那么就算出现重复消费的情况，就会导致主键冲突，避免数据库出现脏数据。
    
2.  如果消息是做redis的set的操作，不用解决，因为无论set几次结果都是一样的，set操作本来就算幂等操作。
3.  如果以上两种情况还不行，可以准备一个第三方介质,来做消费记录。以redis为例，给消息分配一个全局id，只要消费过该消息，将<id,message>以K-V形式写入redis。那消费者开始消费前，先去redis中查询有没消费记录即可也就是上面的使用Redis来解决重复消费和幂等性的解决方案。
```
### 消息积压怎末解决
```
毫无疑问,出问题的肯定是消息消费阶段,那么从消费端入手,如何回答呢?
如果是线上突发问题,要临时扩容,增加消费端的数量,与此同时,降级一些非核心的业务。通过扩容和降级承担流量,这是为了表明你对应急问题的处理能力。
其次,才是排查解决异常问题,如通过监控,日志等手段分析是否消费端的业务逻辑代码出现了问题,优化消费端的业务处理逻辑。最后,如果是消费端的处理能力不足,可以通过水平扩容来提供消费端的并发处理能力,但这里有一个考点需要特别注意,那就是在扩容消费者的实例数的同时,必须同步扩容主题Topic的分区数量,确保消费者的实例数和分区数相等。如果消费者的实例数超过了分区数,由于分区是单线程消费,所以这样的扩容就没有效果。
比如在Kafka中,一个Topic可以配置多个Partition (分区),数据会被写入到多个分区中,但在消费的时候,Kafka约定一个分区只能被一个消费者消费,Topic的分区数量决定了消费的能力,所以,可以通过增加分区来提高消费者的处理能力。
```
### 延迟队列
```
延迟队列的产生：延迟队列是消息队列当中的一种队列形式，即消息在发送到消息队列后，经过指定的时间后消息的接收方才能接收到该消息。

延迟队列一般常用于一些特定的业务场景当中，比如前段时间我在写一个关于订单的业务时就需要引入消息队列，业务场景是这样的：用户创建订单成功，需在30分钟内进行付款，如果不付款的话会将该订单设置成失效的订单，然后对一些库存之类的数据进行补偿回退。

最开始我想到的方法就是定时任务轮询的方式，查询当前时间减去指定过期时间之前未支付的订单，然后对其 进行补偿，但这样的操作其实是比较耗费性能的，因为每过一段时间就需要对数据库中的订单数据做查询，数据量不大还好，一旦数据量增多所带来的性能消耗是巨大的。

于是对其进行了一些优化方案，可以在订单创建成功之后发送一条延时消息，经过指定的时间之后接收到该条消息，然后对其进行判断是否支付即可。

  
```
### 总结
```
1.如何确保消息不会丢失? 你要知道一条消息从发送到消费的每个阶段,是否存在丢消息,以及如何监控消息是否丢失,最后才是如何解决问题,方案可以基于“MQ的可靠消息投递”的方式。
2.如何保证消息不被重复消费? 在进行消息补偿的时候,一定会存在重复消息的情况,那么如何实现消费端的幕等性就这道题的考点。
3.如何处理消息积压问题? 这道题的考点就是如何通过MQ实现真正的高性能,回答的思路是,本着解决线上异常为最高优先级,然后通过监控和日志进行排查并优化业务逻辑,最后是扩容消费端和分片的数量。
```
## Redis
### 5种基本类型适用的场景
```
String(字符串)、hash(哈希)、list(列表)、set(集合)、zset(sorted set:有序集合)

应用场景:
String(字符串)
[1.String通常用于保存单个字符串或Json字符串数据
 2.String是二进制安全的，可以将图片文件的内容作为字符串进行存储
 3.计数器(可用于计数，微博数，粉丝数，点击量等)
]

hash(哈希)
[常用于存储一个对象]

list(列表)
[1.对数据量大的集合数据进行测试,譬如:粉丝列表、留言评论等还可以进行分页；
 2.任务队列；
]

set(集合)
[常用于两个集合间的数据计算，交集，并集，差集等。]

zset(有序集合)
[常用于排行榜、积分榜等]

```
### Redis为什么是单线程的	
```
多线程处理会涉及到锁，而多线程处理会涉及到线程切换而消耗CPU。因为CPU不是Redis的瓶颈，Redis的瓶颈最有可能是机器内存或者网络带宽。单线程无法发挥多核CPU性能，不过可以通过在单机开多个redis实例来解决。
```
### Redis的淘汰策略
```
Redis内存淘汰策略一共有8种
volatile-lru：设置了过期时间的key使用LRU算法淘汰；

allkeys-lru：所有key使用LRU算法淘汰；

volatile-lfu：设置了过期时间的key使用LFU算法淘汰；

allkeys-lfu：所有key使用LFU算法淘汰；

volatile-random：设置了过期时间的key使用随机淘汰；

allkeys-random：所有key使用随机淘汰；

volatile-ttl：设置了过期时间的key根据过期时间淘汰，越早过期越早淘汰；

noeviction：默认策略，当内存达到设置的最大值时，所有申请内存的操作都会报错(如set,lpush等)，只读操作如get命令可以正常执行；


LRU、LFU和volatile-ttl都是近似随机算法；
在缓存的内存淘汰策略中有FIFO、LRU、LFU三种，其中LRU和LFU是Redis在使用的。

LRU算法:
LRU（Least Recently Used）表示最近最少使用，该算法根据数据的历史访问记录来进行淘汰数据，其核心思想是“如果数据最近被访问过，那么将来被访问的几率也更高”。

LFU算法
LFU（Least Frequently Used）表示最不经常使用，它是根据数据的历史访问频率来淘汰数据，其核心思想是“如果数据过去被访问多次，那么将来被访问的频率也更高”。

```
### Redis的过期策略
```
过期删除策略:
前面介绍的LRU和LFU算法都是在Redis内存占用满的情况下的淘汰策略，那么当内存没占满时在Redis中过期的key是如何从内存中删除以达到优化内存占用的呢？
在Redis中过期的key不会立刻从内存中删除，而是会同时以下面两种策略进行删除：

惰性删除：当key被访问时检查该key的过期时间，若已过期则删除；已过期未被访问的数据仍保持在内存中，消耗内存资源；

定期删除：每隔一段时间，随机检查设置了过期的key并删除已过期的key；维护定时器消耗CPU资源；

定时删除：在设置key的过期时间的同时，为该key创建一个定时器，让定时器在key的过期时间来临时，对key进行删除

redis设置过期时间：
expire key time(以秒为单位)--这是最常用的方式
setex(String key, int seconds, String value)--字符串独有的方式


Redis每10秒进行一次过期扫描：
1.随机取20个设置了过期策略的key；
2.检查20个key中过期时间中已过期的key并删除；
3.如果有超过25%的key已过期则重复第一步；
这种循环随机操作会持续到过期key可能仅占全部key的25%以下时，并且为了保证不会出现循环过多的情况，默认扫描时间不会超过25ms；


AOF和RDB的过期删除策略:
当Redis中的key已过期未删除时，如果进行RDB和AOF的持久化操作时候会怎么操作呢？

在RDB持久化模式中我们可以使用save和bgsave命令进行数据持久化操作
在AOF持久化模式中使用rewriteaof和bgrewriteaof命令进行持久化操作

这四个命令都不会将过期key持久化到RDB文件或AOF文件中，可以保证重启服务时不会将过期key载入Redis。

为了保证一致性，在AOF持久化模式中，当key过期时候，会同时发送DEL命令给AOF文件和所有节点；
从节点不会主动的删除过期key除非它升级为主节点或收到主节点发来的DEL命令；
```
### Redis的持久化策略
```
RDB自动持久化:
RDB是默认开启的，可以在配置文件中配置save 多少秒 操作次数，如果在这些秒中操作达到这些次数就会自动保存也就是持久化
AOF自动持久化:
AOF是需要手动开启的,可以在配置文件中将appendonly no值改为yes，Append only file只追加写操作，本质就是一个写操作日志

AOF和RDB能够一起使用
可以一起使用，没有任何问题，但是当redis启动的时候优先加载aof文件。一旦加载了aof的话rdb就不会加载了。

AOF持久化3种策略:
1）appendfsync no   redis不去执行持久化操作而是交给OS，最快
2）appendfsync always  只要发生写操作就立即加到aof中，最慢，最安全
3）appendfsync everysec 每秒一次日志的写入，默认的一种策略
```
### 集群架构

```

```

### 缓存穿透

```
缓存穿透  数据库没有   缓存没有   解决方案: 缓存空对象  布隆过滤器
```

```
缓存穿透是指用户查询数据，在数据库没有，自然在缓存中也不会有。这样就导致用户查询的时候，在缓存中找不到对应key的value，每次都要去数据库再查询一遍，然后返回空（相当于进行了两次无用的查询）。这样请求就绕过缓存直接查数据库
```

```
1、缓存空值
如果一个查询返回的数据为空（不管是数据不存在，还是系统故障）我们仍然把这个空结果进行缓存，但它的过期时间会很短，最长不超过5分钟。通过这个设置的默认值存放到缓存，这样第二次到缓存中获取就有值了，而不会继续访问数据库 
2、采用布隆过滤器BloomFilter 优势：占用内存空间很小，位存储；性能特别高，使用key的hash判断key存不存在
将所有可能存在的数据哈希到一个足够大的bitmap中，一个一定不存在的数据会被这个bitmap拦截掉，从而避免了对底层存储系统的查询压力
在缓存之前在加一层BloomFilter，在查询的时候先去BloomFilter去查询key是否存在，如果不存在就直接返回，存在再去查询缓存，缓存中没有再去查询数据库
```

### 缓存击穿
```
缓存击穿  数据库有  缓存没有 解决方案: 使用互斥锁(mutex key)
```

```
在平常高并发的系统中，大量的请求同时查询一个key时，此时这个key正好失效了，就会导致大量的请求都打到数据库上面去。这种现象我们称为缓存击穿
```

```
1.这种解决方案思路比较简单，就是只让一个线程构建缓存，其他线程等待构建缓存的线程执行完，重新从缓存获取数据就可以了。如果是单机，可以用synchronized或者lock来处理，如果是分布式环境可以用分布式锁就可以了（分布式锁，可以用memcache的add, redis的setnx, zookeeper的添加节点操作）。
2.设置热点key缓存永不过期
```

### 缓存雪崩
```
缓存雪崩是指缓存中数据大批量到过期时间，而查询数据量巨大，引起数据库压力过大甚至down机。和缓存击穿不同的是，缓存击穿指并发查同一条数据，缓存雪崩是不同数据都过期了，很多数据都查不到从而查数据库。
```

```
1.缓存数据的过期时间设置随机，防止同一时间大量数据过期现象发生。
2.设置热点数据永远不过期。
3.对用户发送的请求进行限流、降级,短时间范围内牺牲一些客户体验，限制一部分请求访问，降低应用服务器压力，待业务低速运转后再逐步放开访问
4.定期维护（自动+人工）:对即将过期数据做访问量分析，确认是否延时，配合访问量统计，做热点数据的延时
------------------------------------------------------------------------
缓存雪崩就是瞬间过期数据量太大，导致对数据库服务器造成压力。如能够有效避免过期时间集中，可以有效解决雪崩现象的出现（约40%），配合其他策略一起使用，并监控服务器的运行数据，根据运行记录做快速调整。
```
### 缓存预热
```
缓存预热就是系统上线后，将相关的缓存数据直接加载到缓存系统。这样就可以避免在用户请求的时候，先查询数据库，然后再将数据缓存的问题。用户直接查询事先被预热的缓存数据。如果不进行预热， 那么 Redis 初识状态数据为空，系统上线初期，对于高并发的流量，都会访问到数据库中， 对数据库造成流量的压力。
```
### 缓存降级
```
降级的情况，就是缓存失效或者缓存服务挂掉的情况下，我们也不去访问数据库。我们直接访问内存部分数据缓存或者直接返回默认数据。

举个例子:
对于应用的首页，一般是访问量非常大的地方，首页里面往往包含了部分推荐商品的展示信息。这些推荐商品都会放到缓存中进行存储，同时我们为了避免缓存的异常情况，对热点商品数据也存储到了内存中。同时内存中还保留了一些默认的商品信息。


降级一般是有损的操作，所以尽量减少降级对于业务的影响程度。
```
### Redis的集群类型
```
1.主从模式
主数据库可以进行读写操作,当读写操作导致数据变化时会自动将数据同步给从数据库.
从数据库一般都是只读的,并且接收主数据库同步过来的数据.
一个master可以拥有多个slave,但是一个slave只能对应一个master.
slave挂了不影响其他slave的读和master的读和写,重新启动后会将数据从master同步过来.
master挂了以后,不影响slave的读,但redis不再提供写服务, master重启后redis将重新对外提供写服务
master挂了以后,不会在slave节点中重新选一个master·

2.哨兵模式
每个Sentinel (哨兵)进程以每秒钟一次的频率向整个集群中的Master主服务器, Slave从服务器以及其他Sentinel (哨兵)进程发送一个PING 命令。
如果一个实例(instance)距离最后一次有效回复PING 命令的时间超过 down-after-milliseconds选项所指定的值,则这个实例会被Sentinel(哨兵)进程标记为主观下线(SDOWN)
如果一个Master主服务器被标记为主观下线(SDOWN) ,则正在监视这个Master主服务器的所有Sentinel(哨兵)进程要以每秒次的频率确认Master主服务器的确进入了主观下线状态
当有足够数量的Sentinel(哨兵)进程(大于等于配置文件指定的值)在指定的时间范围内确认Master主服务器进入了主观下线状态(SDOWN),则Master主服务器会被标记为客观下线(ODOWN)
在一般情况下,每个Sentinel(哨兵)进程会以每10秒一次的频率向集群中的所有Master主服务器、Slave从服务器发送INFO命令
当Master主服务器被Sentinel (哨兵)进程标记为客观下线(ODOWN)时, Sentinel(哨兵)进程向下线的Master主服务器的所有Slave从服务器发送INFO命令的频率会从10秒一次改为每秒一次。
若没有足够数量的Sentinel (哨兵)进程同意Master主服务器下线, Master主服务器的客观下线状态就会被移除。若Master主服务器重新向Sentinel (哨兵)进程发送PING 命令返回有效回复,Master主服务器的主观下线状态就会被移除。
当master挂了以后, sentinel会在slave中选择一个做为master,并修改它们的配置文件,其他slave的配置文件也会被修改,比如slaveof属性会指向新的master
当使用sentinel模式的时候,客户端就不要直接连接Redis,而是连接sentinel的ip和port,由sentinel来提供具体的可提供服务的Redis实现,这样当master节点挂掉以后,sentinel就会感知并将新的master节点提供给使用者。

3.Redis-Cluster集群
在redis的每一个节点上,都有这么两个东西,一个是插槽(slot),它的的取值范围是:0-16383。还有一个就是cluster.可以理解为是一个集群的插件。们的存取的key到达的时候,redis会根据crc16的算法得出一个结果,然后把结果对16384求余数,这样每个key都会对应一个编号在 0-16383 之间的哈希槽,通过这个值,去找到对应的插槽所对应的节点,然后直接自动跳转到这个对应的节点上进行存取操作
为了保证高可用,redis-cluster集群引入了主从模式,一个主节点对应一个或者多个从节点,当主节点宕机的时候,就会启用从节点。当其它主节点ping一个主节点A时,如果半数以上的主节点与A通止在联网识别并翻详节点A启机了。如果主节点A和它的从节点A1都容机了那么该集群就无法再提供服务了。

所有的节点都是一主一从(也可以是一主多从),其中从不提供服务,仅作为备用
数据按照slot存储分布在多个节点,节点间数据共享,可动态调整数据分布。
不支持同时处理多个key(如MSET/MGET),因为redis需要把key均匀分布在各个节点上,并发量很高的情况下同时创建key-value会降低性能并导致不可预测的行为
不推荐使用redis的事务机制。因为做了数据分片操作。你一个事务中有涉及到多个key操作的时候,这多个key不一定都存储在同一个redis-server上。 因此, redis的事务机制,十分鸡肋。
所有主redis节点彼此网络互联(PING-PONG机制),内部使用二进制协议优化传输速度和带宽。
节点的fail是通过集群中超过半数的节点检测失效时才生效
支持在线增加、删除节点
客户端与redis节点直连,不需要中间代理层,客户端不需要连接集群所有节点,连接集群中任何一个可用节点即可。·主从之间,数据通过异步复制,不保证数据的强一致性
```
## 项目中常见问题

```

```



## Java项目

```
我叫吕世昊，来自于河南驻马店,在网上看到公司招聘的这个岗位,觉得自己比较适合公司的岗位,对自己的发展也有帮助,所以想来这里争取下这份工作
我所掌握的后端技术框架有ssm和springboot,了解过springcloud,掌握mysql,有一定数据设计能力和sql调优经验,掌握redis非关系行数据库,reids缓存,reids的集群搭建技术,并能整合到项目中使用jedis进行相关操作
熟悉常见的消息中间件如activemq和rabbitmq的使用,了解过微信小程序和微信公众号,熟练使用svn,git,maven等项目管理工具,掌握的前端技术有html,css,javascript,ajax,jquery,layui,ztree,poi。了解过vue。了解过一些大数据的生态体系
我在之前的岗位工作2年多,一共做过4个项目,2个微信小程序,一个APP，一个erp项目
我最近做的这个项目是为河南晨曦集团有限公司开发的一个方便用户足不出户就可以在其连锁店内进行商品的选购而推出的一款微信小程序,小程序主要分为4大模块有首页,分类,购物车,我的,后台有用户管理,商品管理,系统管理,分类管理,店铺管理,商家管理等模块
我在公司主要负责的前台模块接口的开发,如轮播图,降价商品,商品类目,热销排行榜,今日秒杀,购物车的开发和维护
参与过后台的商品管理,库存管理,降价管理等功能,协同过人员调试开发,参与系统BUG修改

我们组一共有5个人,一个测试,一个前台
```

